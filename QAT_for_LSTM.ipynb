{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import absl, os, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "# Import tensorflow model optimization, used for quantization-aware training\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# remove annoying logging\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that converts a tensorflow model with LSTMs to TFLite model\n",
    "def convert_LSTM_to_float_TFLite(model_tf, TFLite_target_filename, batch_size=1):\n",
    "    # We need to clearly set the input signature of the Keras model\n",
    "    # As of now, only one dimension can be dynamic. We must fix the batch size\n",
    "    \n",
    "    # create TFLite converter from teh model saved on disk\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model_tf)\n",
    "    # Set the optimization flag to use default quantization\n",
    "    converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "    # Use default TFLite and TF operators\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.SELECT_TF_OPS,tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    # use float as input and output\n",
    "    converter.inference_input_type = tf.int8\n",
    "    converter.inference_output_type = tf.int8\n",
    "\n",
    "    def generate_representative_dataset():\n",
    "            for i in range(1000):\n",
    "                yield([np.float32(x_train[i]).reshape((1,28,28))])\n",
    "    converter.representative_dataset = generate_representative_dataset\n",
    "    \n",
    "    # Convert model\n",
    "    model_tflite = converter.convert()\n",
    "    \n",
    "    open(TFLite_target_filename, \"wb\").write(model_tflite)\n",
    "    print(f\"TFLite model size: {os.path.getsize(TFLite_target_filename)}\")\n",
    "    return model_tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_TFLite(model, X):\n",
    "    x_data = X.copy() # the function quantizes the input, so we must make a copy\n",
    "    # Initialize the TFLite interpreter\n",
    "    interpreter = tf.lite.Interpreter(model_content=model)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    # Quantize input if needed\n",
    "    input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "    if (input_scale, input_zero_point) != (0.0, 0):\n",
    "        x_data = x_data / input_scale + input_zero_point\n",
    "    x_data = x_data.astype(input_details[\"dtype\"])\n",
    "        \n",
    "    \n",
    "    \n",
    "    for x in x_data:\n",
    "        # We need to resize the input shape to fit the dynamic sequence (batch size must be equal to 1)\n",
    "        interpreter.resize_tensor_input(input_details['index'], (1,)+x.shape, strict=True)\n",
    "        interpreter.allocate_tensors()\n",
    "        interpreter.set_tensor(input_details[\"index\"], [x])\n",
    "        interpreter.invoke()\n",
    "        outputs.append(np.copy(interpreter.get_tensor(output_details[\"index\"])))\n",
    "    \n",
    "    \n",
    "    # Dequantize output\n",
    "    outputs = np.array(outputs)\n",
    "    output_scale, output_zero_point = output_details[\"quantization\"]\n",
    "    if (output_scale, output_zero_point) != (0.0, 0):\n",
    "        outputs = outputs.astype(np.float32)\n",
    "        outputs = (outputs - output_zero_point) * output_scale\n",
    "    # todo reshape output into array for each exit\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "# Basic standardization\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Model - no quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 13s 6ms/step - loss: 0.7592 - accuracy: 0.7654\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2673 - accuracy: 0.9240\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2015 - accuracy: 0.9414\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2019 - accuracy: 0.9372\n",
      "test accuracy: 0.9372000098228455\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "  keras.layers.LSTM(20),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(10)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "  x_train,\n",
    "  y_train,\n",
    "  epochs=3,\n",
    ")\n",
    "_, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Training Quantization (PTQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\POV4RT\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:765: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite model size: 15984\n",
      "Test accuracy: 0.9322\n"
     ]
    }
   ],
   "source": [
    "# Post training quantization\n",
    "model_ptq = convert_LSTM_to_float_TFLite(model, \"model_ptq.tflite\")\n",
    "x_pred = np.argmax(predict_TFLite(model_ptq, x_test),axis=-1)\n",
    "print(f\"Test accuracy: {np.nanmean(x_pred.flatten()==y_test.flatten())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Aware Training - LSTM is quantized after training (Option A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 15s 7ms/step - loss: 0.8373 - accuracy: 0.7415\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.3288 - accuracy: 0.9067\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.2247 - accuracy: 0.9359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\POV4RT\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:765: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite model size: 16072\n"
     ]
    }
   ],
   "source": [
    "class NoOpQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "  def get_weights_and_quantizers(self, layer):\n",
    "    return []\n",
    "  def get_activations_and_quantizers(self, layer):\n",
    "    return []\n",
    "  def set_quantize_weights(self, layer, quantize_weights):\n",
    "    pass\n",
    "  def set_quantize_activations(self, layer, quantize_activations):\n",
    "    pass\n",
    "  def get_output_quantizers(self, layer):\n",
    "    return []\n",
    "  def get_config(self):\n",
    "    return {}\n",
    "\n",
    "def annotate_layers(layer):\n",
    "    if isinstance(layer, tf.keras.layers.LSTM):\n",
    "        return tfmot.quantization.keras.quantize_annotate_layer(layer, quantize_config=NoOpQuantizeConfig())\n",
    "    return layer\n",
    "\n",
    "\n",
    "model_qat = keras.Sequential([\n",
    "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "  keras.layers.LSTM(20),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(10)\n",
    "])\n",
    "model_qat.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Annotate layers when copying\n",
    "model_qat = tf.keras.models.clone_model(model_qat,clone_function=annotate_layers,)\n",
    "# model_q.set_weights(model.get_weights())\n",
    "# Specify scope if you use weird Layers (functional API)\n",
    "with tfmot.quantization.keras.quantize_scope({'Multiply': tf.keras.layers.Multiply}):\n",
    "    model_qat = tfmot.quantization.keras.quantize_model(model_qat)\n",
    "\n",
    "model_qat.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_qat.fit(\n",
    "  x_train,\n",
    "  y_train,\n",
    "  epochs=3,\n",
    ")\n",
    "\n",
    "model_qat_skip = convert_LSTM_to_float_TFLite(model_qat, \"model_qat_skip.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9358\n"
     ]
    }
   ],
   "source": [
    "x_pred = np.argmax(predict_TFLite(model_qat_skip, x_test),axis=-1)\n",
    "print(f\"Test accuracy: {np.nanmean(x_pred.flatten()==y_test.flatten())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization-Aware Training (QAT) - Experimental LSTM quantization (Option B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer_1 (QuantizeL  (None, 28, 28)           3         \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " quant_lstm_2 (QuantizeWrapp  (None, 20)               3849      \n",
      " erV2)                                                           \n",
      "                                                                 \n",
      " quant_flatten_2 (QuantizeWr  (None, 20)               1         \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_dense_2 (QuantizeWrap  (None, 10)               215       \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,068\n",
      "Trainable params: 4,050\n",
      "Non-trainable params: 18\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 21s 10ms/step - loss: 0.9420 - accuracy: 0.6928\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 20s 11ms/step - loss: 0.4037 - accuracy: 0.8816\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.2896 - accuracy: 0.9158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\POV4RT\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:765: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite model size: 19456\n"
     ]
    }
   ],
   "source": [
    "LastValueQuantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer\n",
    "MovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer\n",
    "\n",
    "class LSTMQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "    # Configure how to quantize weights.\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "      return [(layer.cell.kernel, LastValueQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False)),\n",
    "              (layer.cell.recurrent_kernel, LastValueQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False)),\n",
    "              ]\n",
    "\n",
    "    # Configure how to quantize activations.\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "      return [(layer.cell.activation, MovingAverageQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False)),\n",
    "              (layer.cell.recurrent_activation, MovingAverageQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False))]\n",
    "\n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "      # Add this line for each item returned in `get_weights_and_quantizers`\n",
    "      # , in the same order\n",
    "      layer.cell.kernel = quantize_weights[0]\n",
    "      layer.cell.recurrent_kernel = quantize_weights[1]\n",
    "\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "      # Add this line for each item returned in `get_activations_and_quantizers`\n",
    "      # , in the same order.\n",
    "      layer.cell.activation = quantize_activations[0]\n",
    "      layer.cell.recurrent_activation = quantize_activations[1]\n",
    "\n",
    "    # Configure how to quantize outputs (may be equivalent to activations).\n",
    "    def get_output_quantizers(self, layer):\n",
    "      # return [(layer.output, MovingAverageQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False))]\n",
    "      return []\n",
    "\n",
    "    def get_config(self):\n",
    "      return {}\n",
    "\n",
    "\n",
    "\n",
    "quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\n",
    "quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model\n",
    "quantize_scope = tfmot.quantization.keras.quantize_scope\n",
    "\n",
    "\n",
    "quant_aware_model = quantize_annotate_model(keras.Sequential([\n",
    "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "  quantize_annotate_layer(keras.layers.LSTM(20, use_bias=False), LSTMQuantizeConfig()),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(10)\n",
    "]))\n",
    "\n",
    "# `quantize_apply` requires mentioning `DefaultDenseQuantizeConfig` with `quantize_scope`\n",
    "# as well as the custom Keras layer.\n",
    "with quantize_scope({'LSTMQuantizeConfig': LSTMQuantizeConfig,'LSTM': keras.layers.LSTM}):\n",
    "  # Use `quantize_apply` to actually make the model quantization aware.\n",
    "  quant_aware_model = tfmot.quantization.keras.quantize_apply(quant_aware_model)\n",
    "\n",
    "\n",
    "quant_aware_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "quant_aware_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "quant_aware_model.fit(\n",
    "  x_train,\n",
    "  y_train,\n",
    "  epochs=3,\n",
    ")\n",
    "\n",
    "model_qat_noskip = convert_LSTM_to_float_TFLite(quant_aware_model, \"model_qat_noskip.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9233\n"
     ]
    }
   ],
   "source": [
    "x_pred = np.argmax(predict_TFLite(model_qat_noskip, x_test),axis=-1)\n",
    "print(f\"Test accuracy: {np.nanmean(x_pred.flatten()==y_test.flatten())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 64-bit ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e2ba188c55241293ec78af7ed1744f0a8fa3e8e5c8f4742771176210cff0e3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
